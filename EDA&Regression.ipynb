{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns:\n",
    "- <b> Age: </b> Age in years\n",
    "- <b> KM: </b> Accumulated Kilometers on odometer\n",
    "- <b> FuelType: </b> Fuel Type (Petrol, Diesel, CNG)\n",
    "- <b> HP: </b> Horse Power\n",
    "- <b> MetColor: </b> Metallic Color? (Yes=1, No=0)\n",
    "- <b> Automatic: </b> Automatic ( (Yes=1, No=0)\n",
    "- <b> CC: </b> Cylinder Volume in cubic centimeters\n",
    "- <b> Doors: </b> Number of doors\n",
    "- <b> Weight: </b> Weight in Kilograms\n",
    "- <b> Price: </b> Offer Price in EUROs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import Counter\n",
    "from IPython.core.display import display, HTML\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ToyotaCorolla dataset from a CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('./data/ToyotaCorolla.csv')\n",
    "\n",
    "# Displaying the first few rows of the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing/null values in the DataFrame \"df\"\n",
    "# Using the `isnull()` method on the DataFrame, which returns a boolean DataFrame\n",
    "# Summing the number of True values (missing/null) for each column using the `sum()` method\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "#Plot figsize\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#Generate Heat Map, allow annotations and place floats in map\n",
    "sns.heatmap(corr, cmap='magma', annot=True, fmt=\".2f\")\n",
    "#Apply xticks\n",
    "plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "#Apply yticks\n",
    "plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "#show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(12,8))\n",
    "\n",
    "sns.regplot(x = 'Price', y = 'Age', data = df, scatter_kws={'alpha':0.6}, ax = axes[0,0])\n",
    "axes[0,0].set_xlabel('Price', fontsize=14)\n",
    "axes[0,0].set_ylabel('Age', fontsize=14)\n",
    "axes[0,0].yaxis.tick_left()\n",
    "\n",
    "sns.regplot(x = 'Price', y = 'KM', data = df, scatter_kws={'alpha':0.6}, ax = axes[0,1])\n",
    "axes[0,1].set_xlabel('Price', fontsize=14)\n",
    "axes[0,1].set_ylabel('KM', fontsize=14)\n",
    "axes[0,1].yaxis.set_label_position(\"right\")\n",
    "axes[0,1].yaxis.tick_right()\n",
    "\n",
    "sns.regplot(x = 'Price', y = 'Weight', data = df, scatter_kws={'alpha':0.6}, ax = axes[1,0])\n",
    "axes[1,0].set_xlabel('Price', fontsize=14)\n",
    "axes[1,0].set_ylabel('Weight', fontsize=14)\n",
    "\n",
    "sns.regplot(x = 'Price', y = 'HP', data = df, scatter_kws={'alpha':0.6}, ax = axes[1,1])\n",
    "axes[1,1].set_xlabel('Price', fontsize=14)\n",
    "axes[1,1].set_ylabel('HP', fontsize=14)\n",
    "axes[1,1].yaxis.set_label_position(\"right\")\n",
    "axes[1,1].yaxis.tick_right()\n",
    "axes[1,1].set(ylim=(40,160))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1,2,figsize=(14,4))\n",
    "\n",
    "sns.distplot(df['KM'], ax = axes[0])\n",
    "axes[0].set_xlabel('KM', fontsize=14)\n",
    "axes[0].set_ylabel('Count', fontsize=14)\n",
    "axes[0].yaxis.tick_left()\n",
    "\n",
    "sns.scatterplot(x = 'Price', y = 'KM', data = df, ax = axes[1])\n",
    "axes[1].set_xlabel('Price', fontsize=14)\n",
    "axes[1].set_ylabel('KM', fontsize=14)\n",
    "axes[1].yaxis.set_label_position(\"right\")\n",
    "axes[1].yaxis.tick_right()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables using one-hot encoding\n",
    "# The `get_dummies()` function from the pandas library is used to convert categorical variables into binary columns\n",
    "# The function takes a DataFrame `df` as input and returns a new DataFrame with one-hot encoded columns\n",
    "df = pd.get_dummies(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into features (X) and target variable (y)\n",
    "# The 'Price' column is dropped from the DataFrame to create the feature matrix X\n",
    "X = df.drop('Price', axis=1).values\n",
    "\n",
    "# The target variable is assigned to a separate variable y\n",
    "# Here, the first column of the DataFrame 'df' is selected using iloc[:, 0]\n",
    "# The values are then reshaped to have a single column using reshape(-1, 1)\n",
    "y = df.iloc[:, 0].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train: \",X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \",y_train.shape)\n",
    "print(\"Shape of y_test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the LinearRegression class from the scikit-learn library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creating an instance of the LinearRegression class\n",
    "regressor_linear = LinearRegression()\n",
    "\n",
    "# Training the linear regression model on the training data\n",
    "# The fit() method is used to train the model by fitting it to the feature matrix X_train and the target variable y_train\n",
    "regressor_linear.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regressor_linear.score(X_train, y_train))\n",
    "print(regressor_linear.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a widely used technique to assess the generalization performance of regression models (or other predictive models). It helps in understanding how the results of a statistical analysis will generalize to an independent data set. Here's an explanation of how it works:\n",
    "\n",
    "1. Partition the Dataset\n",
    "The dataset is usually divided into 'k' equal-sized 'folds' or 'subsets'. A common choice is k=10, known as 10-fold cross-validation.\n",
    "\n",
    "2. Training and Validation Process\n",
    "For each of the 'k' folds:\n",
    "\n",
    "Training: (k-1) folds are combined to form a training set, and the model is trained on this combined data.\n",
    "Validation: The remaining one fold (left out fold) is used as a validation set to test the model.\n",
    "Performance Metric: The error metric (like Mean Squared Error for regression) is computed for this iteration.\n",
    "3. Repeat the Process\n",
    "This process is repeated k times, with each of the k subsets serving exactly once as the validation data.\n",
    "\n",
    "4. Average the Errors\n",
    "The k results from the folds can then be averaged to produce a single estimation of performance. This helps in reducing the bias, as we are using most of the data for fitting, and also in reducing the variance, as most of the data is also being used in validation.\n",
    "\n",
    "5. Assess the Model\n",
    "The average error is used to assess the model's quality. This gives a more accurate estimate of how well the model has been trained to unseen data.\n",
    "\n",
    "6. Final Model Training\n",
    "Once the cross-validation process is complete and the best hyperparameters are selected, the final model is usually trained on the entire dataset before making predictions on new/unseen data.\n",
    "\n",
    "Use in Hyperparameter Tuning\n",
    "Cross-validation can also be used in conjunction with grid search or other search algorithms to find the optimal hyperparameters for the model.\n",
    "\n",
    "Advantages and Disadvantages\n",
    "Advantages: More reliable estimate of out-of-sample performance compared to train-test split.\n",
    "Disadvantages: Computationally more expensive as it requires fitting and predicting k times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary library or module for cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Predicting the cross-validation score for the test set results\n",
    "# The cross_val_score() function is used to evaluate the performance of the model using cross-validation\n",
    "# The estimator parameter takes the trained regression model 'regressor_linear' as input\n",
    "# The X parameter represents the feature matrix X_train\n",
    "# The y parameter represents the target variable y_train\n",
    "# The cv parameter specifies the number of folds or subsets to be created for cross-validation (in this case, 10)\n",
    "cv_linear = cross_val_score(estimator=regressor_linear, X=X_train, y=y_train, cv=10)\n",
    "\n",
    "# Printing the mean of the cross-validation scores\n",
    "print(\"CV: \", cv_linear.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise: Use another Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
